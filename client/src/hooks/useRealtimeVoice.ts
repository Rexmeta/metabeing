/// <reference types="@types/dom-speech-recognition" />
import { useState, useEffect, useRef, useCallback } from 'react';

export type RealtimeVoiceStatus = 
  | 'disconnected' 
  | 'connecting' 
  | 'connected' 
  | 'error';

interface UseRealtimeVoiceProps {
  conversationId: string;
  scenarioId: string;
  personaId: string;
  personaRunId: string; // chatMessages ÌÖåÏù¥Î∏îÏóê Ï†ÄÏû•ÌïòÍ∏∞ ÏúÑÌïú personaRunId
  enabled: boolean;
  onMessage?: (message: string) => void;
  onMessageComplete?: (message: string, emotion?: string, emotionReason?: string) => void;
  onUserTranscription?: (transcript: string) => void;
  onUserMessageSaved?: (text: string, turnIndex: number) => void; // ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï†ÄÏû• ÏïåÎ¶º
  onError?: (error: string) => void;
  onSessionTerminated?: (reason: string) => void;
}

interface UseRealtimeVoiceReturn {
  status: RealtimeVoiceStatus;
  isRecording: boolean;
  isAISpeaking: boolean;
  connect: (hasExistingMessages?: boolean) => Promise<void>;
  disconnect: () => void;
  startRecording: () => void;
  stopRecording: () => void;
  sendTextMessage: (text: string) => void;
  error: string | null;
}

export function useRealtimeVoice({
  conversationId,
  scenarioId,
  personaId,
  personaRunId,
  enabled,
  onMessage,
  onMessageComplete,
  onUserTranscription,
  onUserMessageSaved,
  onError,
  onSessionTerminated,
}: UseRealtimeVoiceProps): UseRealtimeVoiceReturn {
  const [status, setStatus] = useState<RealtimeVoiceStatus>('disconnected');
  const [isRecording, setIsRecording] = useState(false);
  const [isAISpeaking, setIsAISpeaking] = useState(false);
  const [error, setError] = useState<string | null>(null);
  
  const wsRef = useRef<WebSocket | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const playbackContextRef = useRef<AudioContext | null>(null); // For AI audio playback
  const captureContextRef = useRef<AudioContext | null>(null); // For microphone capture (with echo cancellation)
  const vadContextRef = useRef<AudioContext | null>(null); // For VAD capture (NO echo cancellation)
  const audioChunksRef = useRef<Blob[]>([]);
  const audioProcessorRef = useRef<ScriptProcessorNode | null>(null);
  const vadProcessorRef = useRef<ScriptProcessorNode | null>(null); // VAD processor (no echo cancellation)
  const micStreamRef = useRef<MediaStream | null>(null);
  const rawMicStreamRef = useRef<MediaStream | null>(null); // Raw mic stream for VAD (no echo cancellation)
  const nextPlayTimeRef = useRef<number>(0); // Track when to play next chunk
  const aiMessageBufferRef = useRef<string>(''); // Buffer for AI message transcription
  const isRecordingRef = useRef<boolean>(false); // Ref for recording state (for closures)
  const scheduledSourcesRef = useRef<AudioBufferSourceNode[]>([]); // Track scheduled audio sources for interruption
  const isInterruptedRef = useRef<boolean>(false); // Flag to ignore audio after barge-in until new response
  const expectedTurnSeqRef = useRef<number>(0); // Expected turn sequence for audio filtering
  const voiceActivityStartRef = useRef<number | null>(null); // Timestamp when voice activity started
  const bargeInTriggeredRef = useRef<boolean>(false); // Flag to prevent multiple barge-in triggers
  const serverVoiceDetectedTimeRef = useRef<number | null>(null); // Timestamp when server detected user speaking
  const isAISpeakingRef = useRef<boolean>(false); // Ref for isAISpeaking state (for closures)
  const isAudioPausedRef = useRef<boolean>(false); // Track if AI audio is paused due to user speaking
  const speechRecognitionRef = useRef<SpeechRecognition | null>(null); // Web Speech API for user transcription
  const currentUserTranscriptRef = useRef<string>(''); // Buffer for accumulating user speech
  const recognitionGenRef = useRef<number>(0); // Generation token for SpeechRecognition restarts
  const pendingMessagesRef = useRef<Map<string, {transcript: string, retries: number, sentAt: number}>>(new Map()); // Retry queue with message ID
  const messageIdCounterRef = useRef<number>(0); // Counter for generating unique message IDs
  
  // Store callbacks in refs to avoid recreating connect() on every render
  const onMessageRef = useRef(onMessage);
  const onMessageCompleteRef = useRef(onMessageComplete);
  const onUserTranscriptionRef = useRef(onUserTranscription);
  const onUserMessageSavedRef = useRef(onUserMessageSaved);
  const onErrorRef = useRef(onError);
  const onSessionTerminatedRef = useRef(onSessionTerminated);
  
  useEffect(() => {
    onMessageRef.current = onMessage;
    onMessageCompleteRef.current = onMessageComplete;
    onUserTranscriptionRef.current = onUserTranscription;
    onUserMessageSavedRef.current = onUserMessageSaved;
    onErrorRef.current = onError;
    onSessionTerminatedRef.current = onSessionTerminated;
  }, [onMessage, onMessageComplete, onUserTranscription, onUserMessageSaved, onError, onSessionTerminated]);

  const getWebSocketUrl = useCallback((token: string) => {
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const host = window.location.host;
    return `${protocol}//${host}/api/realtime-voice?conversationId=${conversationId}&scenarioId=${scenarioId}&personaId=${personaId}&personaRunId=${personaRunId}&token=${token}`;
  }, [conversationId, scenarioId, personaId, personaRunId]);

  const getRealtimeToken = useCallback(async (): Promise<string> => {
    // localStorageÏóê authTokenÏù¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
    const storedToken = localStorage.getItem('authToken');
    if (storedToken) {
      console.log('‚úÖ Using stored auth token');
      return storedToken;
    }

    // localStorageÏóê ÏóÜÏúºÎ©¥ realtime-token API Ìò∏Ï∂ú (Ïø†ÌÇ§ Í∏∞Î∞ò Ïù∏Ï¶ù)
    console.log('üîë No stored token, requesting realtime token...');
    try {
      const response = await fetch('/api/auth/realtime-token', {
        method: 'POST',
        credentials: 'include', // Ïø†ÌÇ§ Ìè¨Ìï®
        headers: {
          'Content-Type': 'application/json',
        },
      });

      if (!response.ok) {
        throw new Error('Ïù∏Ï¶ù ÌÜ†ÌÅ∞ÏùÑ Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§. Îã§Ïãú Î°úÍ∑∏Ïù∏Ìï¥Ï£ºÏÑ∏Ïöî.');
      }

      const data = await response.json();
      console.log('‚úÖ Realtime token received, expires in:', data.expiresIn, 'seconds');
      return data.token;
    } catch (error) {
      console.error('‚ùå Failed to get realtime token:', error);
      throw new Error('Ïù∏Ï¶ù ÌÜ†ÌÅ∞ÏùÑ Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§. Îã§Ïãú Î°úÍ∑∏Ïù∏Ìï¥Ï£ºÏÑ∏Ïöî.');
    }
  }, []);

  // Stop all scheduled audio playback immediately (for barge-in/interruption)
  const stopCurrentPlayback = useCallback(() => {
    console.log('üîá Stopping current AI audio playback (barge-in)');
    
    // Set interrupted flag to ignore incoming audio chunks until new response
    isInterruptedRef.current = true;
    
    // Stop all scheduled audio sources
    for (const source of scheduledSourcesRef.current) {
      try {
        source.stop();
        source.disconnect();
      } catch (err) {
        // Source may have already finished playing - log for debugging but continue
        console.warn('Failed to stop audio source (may have already finished):', err);
      }
    }
    scheduledSourcesRef.current = [];
    
    // Suspend and close playback AudioContext to immediately halt all audio
    // This ensures no queued audio chunks can play
    // Note: Only close playback context, keep capture context intact for microphone
    if (playbackContextRef.current && playbackContextRef.current.state !== 'closed') {
      try {
        // Suspend immediately stops all processing
        playbackContextRef.current.suspend();
        // Close and create fresh context for next playback
        playbackContextRef.current.close();
        playbackContextRef.current = null;
        console.log('üîá Playback AudioContext closed to flush audio queue');
      } catch (err) {
        console.warn('Error closing playback AudioContext:', err);
      }
    }
    
    // Reset playback timing
    nextPlayTimeRef.current = 0;
    
    // Reset AI message buffer
    aiMessageBufferRef.current = '';
    
    setIsAISpeaking(false);
    isAISpeakingRef.current = false;
  }, []);

  const disconnect = useCallback(() => {
    // Stop any playing audio first
    stopCurrentPlayback();
    
    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
    }
    if (playbackContextRef.current) {
      playbackContextRef.current.close();
      playbackContextRef.current = null;
    }
    if (captureContextRef.current) {
      captureContextRef.current.close();
      captureContextRef.current = null;
    }
    if (vadContextRef.current) {
      vadContextRef.current.close();
      vadContextRef.current = null;
    }
    // Stop raw microphone stream
    if (rawMicStreamRef.current) {
      rawMicStreamRef.current.getTracks().forEach(track => track.stop());
      rawMicStreamRef.current = null;
    }
    setStatus('disconnected');
    setIsRecording(false);
    setIsAISpeaking(false);
  }, [stopCurrentPlayback]);

  const hasExistingMessagesRef = useRef<boolean>(false);

  const connect = useCallback(async (hasExistingMessages: boolean = false) => {
    hasExistingMessagesRef.current = hasExistingMessages;
    setStatus('connecting');
    setError(null);

    try {
      // üîä AudioContext ÏÇ¨Ï†Ñ Ï§ÄÎπÑ (Ï≤´ Ïù∏ÏÇ¨ ÏùåÏÑ± ÎàÑÎùΩ Î∞©ÏßÄ)
      // ÏÇ¨Ïö©ÏûêÍ∞Ä "Ïó∞Í≤∞" Î≤ÑÌäºÏùÑ ÌÅ¥Î¶≠Ìïú ÏãúÏ†êÏóê AudioContextÎ•º ÎØ∏Î¶¨ ÏÉùÏÑ±ÌïòÍ≥† resume
      if (!playbackContextRef.current || playbackContextRef.current.state === 'closed') {
        playbackContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
        console.log('üîä Pre-created playback AudioContext for first greeting');
      }
      
      // Î∏åÎùºÏö∞Ï†Ä ÏûêÎèôÏû¨ÏÉù Ï†ïÏ±Ö Ìï¥Ï†ú (ÏÇ¨Ïö©Ïûê ÏÉÅÌò∏ÏûëÏö© ÏãúÏ†êÏóê resume)
      if (playbackContextRef.current.state === 'suspended') {
        try {
          await playbackContextRef.current.resume();
          console.log('üîä AudioContext resumed for first greeting playback');
        } catch (err) {
          console.warn('‚ö†Ô∏è Failed to resume AudioContext:', err);
        }
      }
      
      // ÌÜ†ÌÅ∞ Í∞ÄÏ†∏Ïò§Í∏∞ (localStorage ÎòêÎäî realtime-token API)
      const token = await getRealtimeToken();
      console.log('üîë Token obtained for WebSocket');
      
      const url = getWebSocketUrl(token);
      console.log('üåê WebSocket URL:', url);
      
      const ws = new WebSocket(url);
      wsRef.current = ws;

      ws.onopen = () => {
        console.log('üéôÔ∏è WebSocket connected for realtime voice');
        setStatus('connected');
        
        // üîÑ Ïû¨Ïó∞Í≤∞ Ïãú pending Î©îÏãúÏßÄ flush (Ï†ÄÏû• ÌôïÏù∏ Î™ª Î∞õÏùÄ Î©îÏãúÏßÄ Ïû¨Ï†ÑÏÜ°)
        if (pendingMessagesRef.current.size > 0) {
          console.log(`üîÑ Flushing ${pendingMessagesRef.current.size} pending messages after reconnect`);
          pendingMessagesRef.current.forEach((pending, msgId) => {
            if (ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({
                type: 'user.message',
                transcript: pending.transcript,
                messageId: msgId
              }));
              console.log(`üì§ Re-sent pending message: ${msgId}`);
            }
          });
        }
        
        // üîä AudioContext Ï§ÄÎπÑ ÏôÑÎ£å Ïã†Ìò∏ Ï†ÑÏÜ° - ÏÑúÎ≤ÑÎäî Ïù¥ Ïã†Ìò∏Î•º Î∞õÏùÄ ÌõÑ Ï≤´ Ïù∏ÏÇ¨Î•º ÏãúÏûë
        // Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ïò§ÎîîÏò§ Ïû¨ÏÉù Ï§ÄÎπÑÍ∞Ä ÏôÑÎ£åÎêú ÏÉÅÌÉúÏóêÏÑú Ï≤´ Ïù∏ÏÇ¨Î•º Î∞õÏùÑ Ïàò ÏûàÏùå
        setTimeout(() => {
          if (ws.readyState === WebSocket.OPEN) {
            ws.send(JSON.stringify({ type: 'client.ready', hasExistingMessages: hasExistingMessagesRef.current }));
            console.log(`üì§ Sent client.ready signal to server (hasExistingMessages: ${hasExistingMessagesRef.current})`);
            
            // üîß Ïù¥ÎØ∏ Ï¥àÍ∏∞ Î©îÏãúÏßÄÍ∞Ä ÏûàÏúºÎ©¥ AI Ïù∏ÏÇ¨ Ìä∏Î¶¨Í±∞Î•º Í±¥ÎÑàÎúÄ (Ï§ëÎ≥µ Ïù∏ÏÇ¨ Î∞©ÏßÄ)
            if (hasExistingMessagesRef.current) {
              console.log('‚è≠Ô∏è Skipping first greeting trigger - session already has initial messages');
              return;
            }
            
            // üîß Gemini Live APIÎäî Ïò§ÎîîÏò§ ÏûÖÎ†• ÏóÜÏù¥ ÏùëÎãµÌïòÏßÄ ÏïäÏúºÎØÄÎ°ú,
            // ÏßßÏùÄ Î¨¥Ïùå Ïò§ÎîîÏò§ (0.5Ï¥à)Î•º Î≥¥ÎÇ¥ÏÑú AIÍ∞Ä Ï≤´ Ïù∏ÏÇ¨Î•º ÏãúÏûëÌïòÎèÑÎ°ù Ìä∏Î¶¨Í±∞
            setTimeout(() => {
              if (ws.readyState === WebSocket.OPEN) {
                // 16kHz PCM16 Î¨¥Ïùå Ïò§ÎîîÏò§ ÏÉùÏÑ± (0.5Ï¥à = 8000 ÏÉòÌîå)
                const silenceSamples = 8000;
                const silenceBuffer = new Int16Array(silenceSamples);
                // ÏôÑÏ†ÑÌïú Î¨¥Ïùå ÎåÄÏã† ÏïÑÏ£º ÏûëÏùÄ ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä (VAD Ìä∏Î¶¨Í±∞ Î∞©ÏßÄ)
                for (let i = 0; i < silenceSamples; i++) {
                  silenceBuffer[i] = Math.floor(Math.random() * 10) - 5; // -5 to 5 range
                }
                
                // ArrayBuffer to Base64 Î≥ÄÌôò
                const bytes = new Uint8Array(silenceBuffer.buffer);
                let binary = '';
                for (let i = 0; i < bytes.byteLength; i++) {
                  binary += String.fromCharCode(bytes[i]);
                }
                const base64Silence = btoa(binary);
                
                ws.send(JSON.stringify({
                  type: 'input_audio_buffer.append',
                  audio: base64Silence,
                }));
                console.log('üì§ Sent silence audio to trigger first greeting');
                
                // END_OF_TURN Ïù¥Î≤§Ìä∏ Ï†ÑÏÜ°ÏúºÎ°ú AI ÏùëÎãµ Ìä∏Î¶¨Í±∞
                ws.send(JSON.stringify({ type: 'input_audio_buffer.commit' }));
                ws.send(JSON.stringify({ type: 'response.create' }));
                console.log('üì§ Sent END_OF_TURN to trigger AI greeting');
              }
            }, 200); // 200ms ÌõÑ Î¨¥Ïùå Ïò§ÎîîÏò§ Ï†ÑÏÜ°
          }
        }, 100); // 100ms ÎîúÎ†àÏù¥Î°ú WebSocket ÏïàÏ†ïÌôî ÌõÑ Ï†ÑÏÜ°
      };

      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          console.log('üì® WebSocket message:', data.type);

          switch (data.type) {
            case 'session.created':
              console.log('‚úÖ Session created:', data.session);
              break;

            case 'conversation.item.created':
              console.log('üí¨ Conversation item created:', data.item);
              break;

            // üé§ ÏÇ¨Ïö©Ïûê ÏùåÏÑ± Ï†ÑÏÇ¨ (ÌÖçÏä§Ìä∏ Î≥ÄÌôò)
            case 'user.transcription':
              if (data.transcript && onUserTranscriptionRef.current) {
                console.log('üé§ User said:', data.transcript);
                onUserTranscriptionRef.current(data.transcript);
              }
              // Reset server voice detection after transcription is complete
              serverVoiceDetectedTimeRef.current = null;
              break;
            
            // üéôÔ∏è ÏÑúÎ≤ÑÏóêÏÑú ÏÇ¨Ïö©Ïûê ÏùåÏÑ± Í∞êÏßÄ ÏãúÏûë (barge-inÏö©)
            case 'user.speaking.started':
              console.log('üéôÔ∏è Server detected user speaking');
              if (serverVoiceDetectedTimeRef.current === null) {
                serverVoiceDetectedTimeRef.current = Date.now();
              }
              // Check for barge-in after 1.5 seconds
              if (isAISpeakingRef.current && !bargeInTriggeredRef.current) {
                setTimeout(() => {
                  // Double-check conditions after delay
                  if (isAISpeakingRef.current && !bargeInTriggeredRef.current && serverVoiceDetectedTimeRef.current !== null) {
                    const duration = Date.now() - serverVoiceDetectedTimeRef.current;
                    if (duration >= 1500) {
                      console.log('üé§ 1.5-second voice detected by server - triggering barge-in');
                      bargeInTriggeredRef.current = true;
                      
                      // Stop current AI audio playback
                      stopCurrentPlayback();
                      
                      // Increment expected turn seq to ignore audio from cancelled turn
                      expectedTurnSeqRef.current++;
                      
                      // Send cancel signal to server
                      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
                        wsRef.current.send(JSON.stringify({
                          type: 'response.cancel',
                        }));
                        console.log('üì§ Sent response.cancel after 1.5-second voice detection');
                      }
                    }
                  }
                }, 1500);
              }
              break;

            // üîä Ïò§ÎîîÏò§ Ïû¨ÏÉù
            case 'audio.delta':
              if (data.delta) {
                // Filter by turn sequence if provided
                if (data.turnSeq !== undefined && data.turnSeq <= expectedTurnSeqRef.current) {
                  console.log(`üîá Ignoring old audio (turnSeq ${data.turnSeq} <= expected ${expectedTurnSeqRef.current})`);
                  break;
                }
                setIsAISpeaking(true);
                isAISpeakingRef.current = true;
                playAudioDelta(data.delta);
              }
              break;

            case 'audio.done':
              console.log('‚úÖ Audio playback complete');
              break;

            // üìù AI ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç (Î≤ÑÌçºÏóê ÎàÑÏ†Å)
            case 'ai.transcription.delta':
              if (data.text) {
                aiMessageBufferRef.current += data.text;
                // Ïã§ÏãúÍ∞Ñ Ïä§Ìä∏Î¶¨Î∞ç ÌëúÏãúÏö© (ÏÑ†ÌÉùÏ†Å)
                if (onMessageRef.current) {
                  onMessageRef.current(data.text);
                }
              }
              break;

            case 'ai.transcription.done':
              console.log('‚úÖ Transcription complete:', data.text);
              console.log('üòä Emotion:', data.emotion, '|', data.emotionReason);
              // ÏôÑÏ†ÑÌïú Î©îÏãúÏßÄÏôÄ Í∞êÏ†ï Ï†ïÎ≥¥Î•º onMessageCompleteÎ°ú Ï†ÑÎã¨
              if (data.text && onMessageCompleteRef.current) {
                onMessageCompleteRef.current(data.text, data.emotion, data.emotionReason);
              }
              // Î≤ÑÌçº Ï¥àÍ∏∞Ìôî
              aiMessageBufferRef.current = '';
              break;

            case 'response.done':
              console.log('‚úÖ Response complete');
              setIsAISpeaking(false);
              isAISpeakingRef.current = false;
              // Do NOT reset interrupted flag here - wait for response.started from a genuine new turn
              break;

            case 'response.interrupted':
              console.log('‚ö° Response interrupted (barge-in acknowledged)');
              setIsAISpeaking(false);
              isAISpeakingRef.current = false;
              // Keep interrupted flag true until user finishes speaking and new response starts
              break;

            case 'response.ready':
              // Server confirms previous turn complete, update expected turn seq
              console.log('üîä Previous turn complete, clearing barge-in flag');
              isInterruptedRef.current = false;
              bargeInTriggeredRef.current = false; // Reset barge-in trigger for next interaction
              serverVoiceDetectedTimeRef.current = null; // Reset server voice detection
              if (data.turnSeq !== undefined) {
                expectedTurnSeqRef.current = data.turnSeq - 1; // Accept audio from this turn onwards
              }
              break;

            case 'user.message.saved':
              console.log('üíæ User message saved:', data.text || data.transcript, 'turnIndex:', data.turnIndex, 'messageId:', data.messageId);
              // Ïû¨ÏãúÎèÑ ÌÅêÏóêÏÑú Ìï¥Îãπ Î©îÏãúÏßÄ Ï†úÍ±∞ (messageId Í∏∞Î∞ò)
              if (data.messageId && pendingMessagesRef.current.has(data.messageId)) {
                pendingMessagesRef.current.delete(data.messageId);
                console.log('‚úÖ Message confirmed and removed from pending:', data.messageId);
              }
              // UI ÏóÖÎç∞Ïù¥Ìä∏ ÏΩúÎ∞± Ìò∏Ï∂ú (ÏÑúÎ≤Ñ VAD Ï†ÄÏû• Í≤ΩÎ°úÏóêÏÑú ÏÇ¨Ïö©)
              if (onUserMessageSavedRef.current && (data.text || data.transcript)) {
                onUserMessageSavedRef.current(data.text || data.transcript, data.turnIndex || 0);
              }
              break;

            case 'user.message.failed':
              console.error('‚ùå User message save failed:', data.transcript, data.messageId, data.error);
              // messageIdÎ°ú pending Î©îÏãúÏßÄ Ï∞æÏïÑÏÑú Ïû¨ÏãúÎèÑ
              const msgId = data.messageId;
              if (msgId && pendingMessagesRef.current.has(msgId)) {
                const pending = pendingMessagesRef.current.get(msgId)!;
                pending.retries++;
                if (pending.retries >= 3) {
                  console.error('‚ùå Max retries reached for:', msgId, pending.transcript);
                  pendingMessagesRef.current.delete(msgId);
                  if (onErrorRef.current) {
                    onErrorRef.current('Î©îÏãúÏßÄ Ï†ÄÏû•Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.');
                  }
                } else {
                  // 1Ï¥à ÌõÑ Ïû¨ÏãúÎèÑ
                  setTimeout(() => {
                    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
                      wsRef.current.send(JSON.stringify({ 
                        type: 'user.message', 
                        transcript: pending.transcript,
                        messageId: msgId 
                      }));
                      console.log(`üîÑ Retrying message (attempt ${pending.retries}):`, msgId);
                    }
                  }, 1000 * pending.retries); // Ï†êÏßÑÏ†Å Î∞±Ïò§ÌîÑ
                }
              }
              break;

            case 'ai.message.saved':
              console.log('üíæ AI message saved:', data.text?.substring(0, 50));
              break;

            case 'ai.message.failed':
              console.error('‚ùå AI message save failed:', data.text?.substring(0, 50), data.error);
              break;

            case 'session.terminated':
              console.log('üîå Session terminated:', data.reason);
              if (onSessionTerminatedRef.current) {
                onSessionTerminatedRef.current(data.reason || 'Session ended');
              }
              disconnect();
              break;

            case 'error':
              console.error('‚ùå Server error:', data.error);
              setError(data.error);
              if (onErrorRef.current) {
                onErrorRef.current(data.error);
              }
              break;

            default:
              console.log('üì® Unhandled message type:', data.type);
          }
        } catch (err) {
          console.error('Error parsing WebSocket message:', err);
        }
      };

      ws.onerror = (event) => {
        console.error('‚ùå WebSocket error:', event);
        setError('WebSocket connection error');
        setStatus('error');
        if (onErrorRef.current) {
          onErrorRef.current('Connection error');
        }
      };

      ws.onclose = (event) => {
        console.log('üîå WebSocket closed:', event.code, event.reason);
        setStatus('disconnected');
        setIsRecording(false);
      };

    } catch (err) {
      console.error('Error connecting to WebSocket:', err);
      setError(err instanceof Error ? err.message : 'Connection failed');
      setStatus('error');
      if (onErrorRef.current) {
        onErrorRef.current(err instanceof Error ? err.message : 'Connection failed');
      }
    }
  }, [enabled, getRealtimeToken, getWebSocketUrl, disconnect]);

  const playAudioDelta = useCallback(async (base64Audio: string) => {
    // Ignore audio chunks if interrupted (barge-in active)
    if (isInterruptedRef.current) {
      console.log('üîá Ignoring audio chunk (barge-in active)');
      return;
    }
    
    try {
      if (!playbackContextRef.current) {
        playbackContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
        nextPlayTimeRef.current = 0; // Reset play time
        console.log('üîä Created new playback AudioContext');
      }

      const audioContext = playbackContextRef.current;
      
      // Resume AudioContext if suspended (browser autoplay policy)
      // This is critical for first greeting audio to play
      if (audioContext.state === 'suspended') {
        console.log('üîä Resuming suspended AudioContext for playback');
        await audioContext.resume();
      }
      
      // Decode base64 to raw bytes
      const binaryString = atob(base64Audio);
      const len = binaryString.length;
      const audioData = new Uint8Array(len);
      for (let i = 0; i < len; i++) {
        audioData[i] = binaryString.charCodeAt(i);
      }
      
      // Convert PCM16 (Int16) to Float32 for Web Audio API
      const pcm16 = new Int16Array(audioData.buffer);
      const float32 = new Float32Array(pcm16.length);
      
      // Normalize PCM16 values (-32768 to 32767) to Float32 (-1.0 to 1.0)
      for (let i = 0; i < pcm16.length; i++) {
        float32[i] = pcm16[i] / 32768.0;
      }

      // Create AudioBuffer for Gemini's 24kHz output
      const audioBuffer = audioContext.createBuffer(1, float32.length, 24000);
      audioBuffer.getChannelData(0).set(float32);
      
      // Calculate when to play this chunk (sequential playback)
      const currentTime = audioContext.currentTime;
      const startTime = Math.max(currentTime, nextPlayTimeRef.current);
      
      // Play audio at scheduled time
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      
      // Î∞úÌôî ÏÜçÎèÑÎ•º 10% ÎäêÎ¶¨Í≤å ÏÑ§Ï†ï (0.9Î∞∞ ÏÜçÎèÑ - Îçî ÏûêÏó∞Ïä§ÎüΩÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨ÏõÄ)
      source.playbackRate.value = 0.9;
      
      source.connect(audioContext.destination);
      source.start(startTime);
      
      // Track source for potential interruption (barge-in)
      scheduledSourcesRef.current.push(source);
      
      // Clean up finished sources
      source.onended = () => {
        const index = scheduledSourcesRef.current.indexOf(source);
        if (index > -1) {
          scheduledSourcesRef.current.splice(index, 1);
        }
      };
      
      // Update next play time (current chunk start time + duration / playbackRate)
      nextPlayTimeRef.current = startTime + (audioBuffer.duration / 0.9);
      
      console.log('üîä Playing audio chunk:', float32.length, 'samples', 'at', startTime.toFixed(3));
    } catch (err) {
      console.error('Error playing audio delta:', err);
    }
  }, []);

  const startRecording = useCallback(async () => {
    if (status !== 'connected' || !wsRef.current) {
      console.warn('Cannot start recording: not connected');
      return;
    }

    // Barge-in: If AI is speaking, interrupt it
    if (isAISpeaking) {
      console.log('üé§ User starting to speak - interrupting AI (barge-in)');
      
      // Stop audio playback immediately
      stopCurrentPlayback();
      
      // Send interrupt signal to server to cancel current AI response
      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify({
          type: 'response.cancel',
        }));
        console.log('üì§ Sent response.cancel to server');
      }
    }

    try {
      // Single mic stream - shared between Gemini and VAD
      // Note: We use echo cancellation for clean audio, and VAD uses the same stream
      // since the separate rawStream approach had issues with browser mic access
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 16000, // Gemini Live API expects 16kHz input
          echoCancellation: true,
          noiseSuppression: true,
        } 
      });
      
      micStreamRef.current = stream;
      console.log('üéôÔ∏è Created single mic stream for Gemini + VAD');

      // Create AudioContext for PCM16 conversion
      if (!captureContextRef.current) {
        captureContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      }

      const audioContext = captureContextRef.current;
      const source = audioContext.createMediaStreamSource(stream);
      
      console.log(`üéôÔ∏è AudioContext sample rate: ${audioContext.sampleRate}Hz`);
      
      // VAD Processor: Uses same stream for voice activity detection
      const vadProcessor = audioContext.createScriptProcessor(4096, 1, 1);
      vadProcessorRef.current = vadProcessor;
      
      vadProcessor.onaudioprocess = (e) => {
        if (!isRecordingRef.current) return;
        
        const inputData = e.inputBuffer.getChannelData(0);
        
        // Calculate RMS for voice activity detection
        let sum = 0;
        for (let i = 0; i < inputData.length; i++) {
          sum += inputData[i] * inputData[i];
        }
        const rms = Math.sqrt(sum / inputData.length);
        const VOICE_THRESHOLD = 0.03; // Higher threshold to avoid false triggers from background noise/echo
        const BARGE_IN_DELAY_MS = 300; // Require 300ms of continuous voice before triggering barge-in
        
        // Check if playback AudioContext is actually running (more reliable than isAISpeakingRef)
        const isPlaybackRunning = playbackContextRef.current?.state === 'running';
        
        // Debug logging
        if (Math.random() < 0.08) {
          console.log(`üîä RAW-VAD: RMS=${rms.toFixed(4)}, threshold=${VOICE_THRESHOLD}, playbackRunning=${isPlaybackRunning}`);
        }
        
        if (rms > VOICE_THRESHOLD) {
          // Track voice activity start time
          if (voiceActivityStartRef.current === null) {
            voiceActivityStartRef.current = Date.now();
            console.log('üé§ Voice activity started');
          }
          
          const voiceDuration = Date.now() - voiceActivityStartRef.current;
          
          // Only trigger barge-in after sustained voice activity (reduces false triggers)
          if (voiceDuration >= BARGE_IN_DELAY_MS && !bargeInTriggeredRef.current && isPlaybackRunning) {
            console.log(`üé§ ${BARGE_IN_DELAY_MS}ms voice detected - triggering barge-in`);
            bargeInTriggeredRef.current = true;
            
            // 1. Stop current audio playback and clear buffer
            stopCurrentPlayback();
            
            // 2. Increment expected turn seq to ignore any remaining audio from old response
            expectedTurnSeqRef.current++;
            console.log(`üìä Expected turn seq incremented to ${expectedTurnSeqRef.current}`);
            
            // 3. Send response.cancel to server to stop Gemini from generating more audio
            if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
              wsRef.current.send(JSON.stringify({
                type: 'response.cancel',
              }));
              console.log('üì§ Sent response.cancel to interrupt AI response');
            }
          }
        } else {
          // User stopped speaking - reset barge-in flag for next interruption
          if (bargeInTriggeredRef.current) {
            console.log('üîá User stopped speaking - ready for new AI response');
            bargeInTriggeredRef.current = false;
          }
          voiceActivityStartRef.current = null;
        }
      };
      
      source.connect(vadProcessor);
      const vadDummyGain = audioContext.createGain();
      vadDummyGain.gain.value = 0;
      vadProcessor.connect(vadDummyGain);
      vadDummyGain.connect(audioContext.destination);
      
      // Main Audio Processor: Uses processed stream for Gemini
      const processor = audioContext.createScriptProcessor(4096, 1, 1);
      audioProcessorRef.current = processor;
      
      processor.onaudioprocess = (e) => {
        if (!isRecordingRef.current) return;
        
        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
          return;
        }

        const inputData = e.inputBuffer.getChannelData(0);
        
        // Resample to 16kHz for Gemini Live API
        const targetSampleRate = 16000;
        const sourceSampleRate = audioContext.sampleRate;
        const ratio = sourceSampleRate / targetSampleRate;
        const targetLength = Math.floor(inputData.length / ratio);
        const resampledData = new Float32Array(targetLength);
        
        for (let i = 0; i < targetLength; i++) {
          const sourceIndex = Math.floor(i * ratio);
          resampledData[i] = inputData[sourceIndex];
        }
        
        // Convert Float32 to Int16 (PCM16)
        const pcm16 = new Int16Array(resampledData.length);
        for (let i = 0; i < resampledData.length; i++) {
          const s = Math.max(-1, Math.min(1, resampledData[i]));
          pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        
        // Convert to base64 and send
        const uint8Array = new Uint8Array(pcm16.buffer);
        let binaryString = '';
        for (let i = 0; i < uint8Array.length; i++) {
          binaryString += String.fromCharCode(uint8Array[i]);
        }
        const base64 = btoa(binaryString);
        
        wsRef.current.send(JSON.stringify({
          type: 'input_audio_buffer.append',
          audio: base64,
        }));
        
        if (Math.random() < 0.1) {
          console.log('üé§ Sending audio chunk:', pcm16.length, 'samples');
        }
      };
      
      source.connect(processor);
      const dummyGain = audioContext.createGain();
      dummyGain.gain.value = 0;
      processor.connect(dummyGain);
      dummyGain.connect(audioContext.destination);
      
      setIsRecording(true);
      isRecordingRef.current = true; // Update ref for onaudioprocess callback
      console.log('üé§ Recording started (PCM16 16kHz for Gemini)');
      
      // üé§ Web Speech APIÎ°ú ÏÇ¨Ïö©Ïûê ÏùåÏÑ± Ï†ÑÏÇ¨ ÏãúÏûë
      const SpeechRecognitionClass = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      if (SpeechRecognitionClass) {
        try {
          // üîß Generation tokenÏúºÎ°ú stale Ïù¥Î≤§Ìä∏ Î¨¥Ïãú
          recognitionGenRef.current++;
          const currentGen = recognitionGenRef.current;
          
          // Ïû¨ÏãúÏûë Ìï®Ïàò (generation token ÏÇ¨Ïö©)
          const startRecognitionInstance = (gen: number) => {
            // üîí stale generationÏù¥Î©¥ Î¨¥Ïãú
            if (gen !== recognitionGenRef.current) {
              console.log(`üé§ [STT] Ignoring stale restart (gen ${gen} vs ${recognitionGenRef.current})`);
              return;
            }
            
            try {
              // Ïù¥Ï†Ñ recognition Ï†ïÎ¶¨
              if (speechRecognitionRef.current) {
                try {
                  speechRecognitionRef.current.onend = null;
                  speechRecognitionRef.current.onerror = null;
                  speechRecognitionRef.current.onresult = null;
                  speechRecognitionRef.current.stop();
                } catch (e) { /* ignore */ }
                speechRecognitionRef.current = null;
              }
              
              currentUserTranscriptRef.current = '';
              const recognition = new SpeechRecognitionClass();
              recognition.lang = 'ko-KR';
              recognition.continuous = true;
              recognition.interimResults = true;
              
              recognition.onresult = (event: SpeechRecognitionEvent) => {
                if (gen !== recognitionGenRef.current) return; // stale
                
                let finalTranscript = '';
                let interimTranscript = '';
                
                for (let i = event.resultIndex; i < event.results.length; i++) {
                  const transcript = event.results[i][0].transcript;
                  if (event.results[i].isFinal) {
                    finalTranscript += transcript;
                  } else {
                    interimTranscript += transcript;
                  }
                }
                
                if (finalTranscript.trim()) {
                  console.log('üé§ [STT] Final transcript:', finalTranscript);
                  const msg = finalTranscript.trim();
                  currentUserTranscriptRef.current = '';
                  
                  // Í≥†Ïú† Î©îÏãúÏßÄ ID ÏÉùÏÑ±
                  const msgId = `msg_${Date.now()}_${++messageIdCounterRef.current}`;
                  
                  // pending ÌÅêÏóê Î®ºÏ†Ä Ï∂îÍ∞Ä (ÌôïÏù∏ Ï†ÑÍπåÏßÄ Î≥¥Í¥Ä)
                  pendingMessagesRef.current.set(msgId, { 
                    transcript: msg, 
                    retries: 0, 
                    sentAt: Date.now() 
                  });
                  
                  if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
                    wsRef.current.send(JSON.stringify({ 
                      type: 'user.message', 
                      transcript: msg,
                      messageId: msgId 
                    }));
                    console.log('üì§ Sent user message with ID:', msgId);
                  } else {
                    console.log('üì¶ Message queued for later (WS not open):', msgId);
                  }
                  
                  if (onUserTranscriptionRef.current) {
                    onUserTranscriptionRef.current(msg);
                  }
                }
                
                if (interimTranscript && Math.random() < 0.3) {
                  console.log('üé§ [STT] Interim:', interimTranscript);
                }
              };
              
              recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
                if (gen !== recognitionGenRef.current) return; // stale
                console.warn('üé§ [STT] Error:', event.error);
                currentUserTranscriptRef.current = '';
                if (event.error !== 'no-speech') {
                  console.error('üé§ [STT] Recognition error:', event.error);
                }
              };
              
              recognition.onend = () => {
                if (gen !== recognitionGenRef.current) return; // stale
                console.log('üé§ [STT] Recognition ended');
                currentUserTranscriptRef.current = '';
                if (isRecordingRef.current) {
                  startRecognitionInstance(gen);
                }
              };
              
              recognition.start();
              speechRecognitionRef.current = recognition;
              console.log(`üé§ [STT] Recognition started (gen ${gen})`);
            } catch (e) {
              console.warn('üé§ [STT] Could not start/restart:', e);
            }
          };
          
          startRecognitionInstance(currentGen);
        } catch (e) {
          console.warn('üé§ [STT] Failed to start Web Speech API:', e);
        }
      } else {
        console.warn('üé§ [STT] Web Speech API not supported in this browser');
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      setError('Microphone access denied');
      if (onErrorRef.current) {
        onErrorRef.current('Microphone access denied');
      }
    }
  }, [status, isAISpeaking, stopCurrentPlayback]);

  const stopRecording = useCallback(() => {
    console.log('üé§ Stopping recording...');
    
    // üé§ Web Speech API Ï†ïÎ¶¨
    if (speechRecognitionRef.current) {
      try {
        speechRecognitionRef.current.stop();
        console.log('üé§ [STT] Stopped Web Speech API');
      } catch (e) {
        console.warn('üé§ [STT] Error stopping:', e);
      }
      speechRecognitionRef.current = null;
    }
    
    // Reset voice activity tracking
    voiceActivityStartRef.current = null;
    bargeInTriggeredRef.current = false;
    isAudioPausedRef.current = false;
    
    // Resume audio if it was paused
    if (playbackContextRef.current && playbackContextRef.current.state === 'suspended') {
      playbackContextRef.current.resume().catch(() => {});
    }
    
    // Stop sending audio first
    setIsRecording(false);
    isRecordingRef.current = false; // Update ref to stop onaudioprocess
    
    // Small delay to ensure last audio chunks are sent
    setTimeout(() => {
      // Disconnect audio processor
      if (audioProcessorRef.current) {
        audioProcessorRef.current.disconnect();
        audioProcessorRef.current = null;
      }
      
      // Disconnect VAD processor
      if (vadProcessorRef.current) {
        vadProcessorRef.current.disconnect();
        vadProcessorRef.current = null;
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach(track => track.stop());
        micStreamRef.current = null;
      }
      
      // Stop raw microphone stream (VAD)
      if (rawMicStreamRef.current) {
        rawMicStreamRef.current.getTracks().forEach(track => track.stop());
        rawMicStreamRef.current = null;
      }
      
      // Commit audio and request response
      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
        console.log('üì§ Committing audio buffer and requesting response');
        wsRef.current.send(JSON.stringify({
          type: 'input_audio_buffer.commit',
        }));
        wsRef.current.send(JSON.stringify({
          type: 'response.create',
          response: {
            modalities: ['audio', 'text'],
          },
        }));
      }
      
      console.log('‚úÖ Recording stopped and committed');
    }, 100); // 100ms delay
  }, []);

  const sendTextMessage = useCallback((text: string) => {
    if (!text.trim() || !wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.log('‚ö†Ô∏è Cannot send text message: invalid state');
      return;
    }

    console.log('üì§ Sending text message:', text);

    // Add user transcription to local display
    if (onUserTranscriptionRef.current) {
      onUserTranscriptionRef.current(text);
    }

    // Send text as conversation item to Gemini
    wsRef.current.send(JSON.stringify({
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [
          {
            type: 'input_text',
            text: text,
          }
        ]
      }
    }));

    // Request AI response
    wsRef.current.send(JSON.stringify({
      type: 'response.create',
      response: {
        modalities: ['audio', 'text'],
      },
    }));

    console.log('‚úÖ Text message sent and response requested');
  }, []);

  useEffect(() => {
    if (enabled) {
      connect();
    }
    
    return () => {
      disconnect();
    };
  }, [enabled, connect, disconnect]);

  return {
    status,
    isRecording,
    isAISpeaking,
    connect,
    disconnect,
    startRecording,
    stopRecording,
    sendTextMessage,
    error,
  };
}
